{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68664161-6341-410b-9533-421707d06d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sklearn\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.loggers.base import LightningLoggerBase\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional, List, Dict, Any, Callable\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pl.utilities.seed.seed_everything(42)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece627e-266b-407b-85c8-caead7e3e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def standarize(data):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240d5d9-b20d-4dac-98f0-47b06b0032d7",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897f617-83d6-443e-98b9-ddd62bedc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, test_size: float = 0.3, val_size: float = 0.1, train_batch_size: int = 64, val_batch_size: int = 64, transforms: List[Callable] = [], no_batch: bool = False):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir # Target as last DataFrame column for classification\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.no_batch = no_batch # full dataset in one batch\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.full_data = pd.read_csv(self.data_dir)\n",
    "        X = self.full_data.values[:, :-1]\n",
    "        y = self.full_data.values[:, -1]\n",
    "        for transform in self.transforms:\n",
    "            X = transform(X)\n",
    "        self.transformed_data = X\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, stratify=y)\n",
    "        X_train = torch.tensor(X_train).float()\n",
    "        X_test = torch.tensor(X_test).float()\n",
    "        y_train = torch.tensor(y_train).long()\n",
    "        y_test = torch.tensor(y_test).long()\n",
    "        \n",
    "        self.train_data = (X_train, y_train)\n",
    "        self.test_data = (X_test, y_test)\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            X_train, y_train = self.train_data\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=self.val_size, stratify=y_train)\n",
    "            self.train_data = []\n",
    "            self.val_data = []\n",
    "            for x, y in zip(X_train, y_train):\n",
    "                self.train_data.append((x, y))\n",
    "                \n",
    "            for x, y in zip(X_val, y_val):\n",
    "                self.val_data.append((x, y))\n",
    "            \n",
    "            if self.no_batch:\n",
    "                self.train_batch_size = len(self.train_data)\n",
    "                self.val_batch_size = len(self.val_data)\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            X_test, y_test = self.test_data\n",
    "            self.test_data = []\n",
    "            for x, y in zip(X_test, y_test):\n",
    "                self.test_data.append((x, y))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, self.train_batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, self.val_batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, shuffle=False, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5501693-3006-47bb-b8b6-cf2c53cc742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize data\n",
    "datamodule = DataModule('', no_batch=True, transforms=[normalize])\n",
    "datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d74ca-5dda-4149-b40c-a52015c67dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.full_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22daf67-711d-4138-a463-d216f4290fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63d6cb-b647-4330-941e-280972d5059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.full_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7253d-9f11-4feb-920f-89179e68ce98",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d519ce4-0be9-4874-8726-77a31c5673ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(data, x, y, figsize=(13, 13), bins=15):\n",
    "    fig, ax = plt.subplots(x, y, figsize=figsize)\n",
    "    for i in range(data.transformed_data.shape[1]):\n",
    "        ax[int(i / y), i % y].hist(data.transformed_data[:, i], bins=bins)\n",
    "        ax[int(i / y), i % y].set_title(data.full_data.columns[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bfd15-2b02-4c7f-b1d3-cb29c8fc9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(datamodule, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484dca68-0263-4106-8aeb-b1677a77d652",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeab466-3e37-4d0b-8680-38a2f12e1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def shared_step(self, x, y):\n",
    "        pred = self(x)\n",
    "        loss = self.criterion(pred, y)\n",
    "        return pred, loss\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        pred, loss = self.shared_step(x, y)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        return {'loss': loss, 'train_score': (predicted, y)}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = [output['loss'].detach().cpu().numpy() for output in outputs]\n",
    "        mean_loss = np.mean(loss)\n",
    "        \n",
    "        results = [x['train_score'] for x in outputs]\n",
    "        preds = []\n",
    "        y = []\n",
    "        for predicted, y_data in results:\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            y_data = y_data.detach().cpu().numpy()\n",
    "            preds.extend(predicted)\n",
    "            y.extend(y_data)\n",
    "        f1 = f1_score(y, preds)\n",
    "        self.log('loss', mean_loss, logger=True)\n",
    "        self.log('train_f1', f1, prog_bar=True, logger=True)\n",
    "    \n",
    "    def validation_step(self, val_batch, val_batch_idx):\n",
    "        x, y = val_batch\n",
    "        pred, loss = self.shared_step(x, y)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        return {'val_loss': loss, 'val_score': (predicted, y)}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        results = [x['val_score'] for x in outputs]\n",
    "        preds = []\n",
    "        y = []\n",
    "        for predicted, y_data in results:\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            y_data = y_data.detach().cpu().numpy()\n",
    "            preds.extend(predicted)\n",
    "            y.extend(y_data)\n",
    "        f1 = f1_score(y, preds)\n",
    "        self.log('val_f1', f1, prog_bar=True, logger=True)\n",
    "        \n",
    "    def test_step(self, test_batch, test_batch_idx):\n",
    "        x, y = test_batch\n",
    "        pred, loss = self.shared_step(x, y)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        return {'test_loss': loss, 'test_score': (predicted, y)}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        results = [x['test_score'] for x in outputs]\n",
    "        preds = []\n",
    "        y = []\n",
    "        for predicted, y_data in results:\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            y_data = y_data.detach().cpu().numpy()\n",
    "            preds.extend(predicted)\n",
    "            y.extend(y_data)\n",
    "        acc = accuracy_score(y, preds)\n",
    "        f1 = f1_score(y, preds)\n",
    "        self.log('accuracy', acc, prog_bar=True, logger=True)\n",
    "        self.log('f1', f1, prog_bar=True, logger=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6dc68-5871-4e16-86aa-016b69ac7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nn = None\n",
    "        self.name = '' # File name for model's checkpoint\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4174bb-4ab0-4337-a884-70318c6dfe4a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f13c7d-fdf1-4425-9260-fce8c3e6a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(elem: Any, count: int, step: Any):\n",
    "    \"\"\"\n",
    "    Generates a list of elements with a given step\n",
    "    \"\"\"\n",
    "    current = elem\n",
    "    output = []\n",
    "    for _ in range(count):\n",
    "        output.append(current)\n",
    "        current += step\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850dcb23-5192-4ff9-80dc-b1ae430c0a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLFlowLogger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a7728244bf9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     def __init__(self, models: List[pl.LightningModule], version: str, folder_path: str = './', trainer_params: Dict[str, Any] = {}, logger: LightningLoggerBase = MLFlowLogger, logger_params: Dict[str, Any] = {}, \n\u001b[0;32m      3\u001b[0m                  callbacks: List[Callback] = [], callbacks_params: List[Dict[str, Any]] = [{}]):\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-a7728244bf9a>\u001b[0m in \u001b[0;36mEvaluator\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     def __init__(self, models: List[pl.LightningModule], version: str, folder_path: str = './', trainer_params: Dict[str, Any] = {}, logger: LightningLoggerBase = MLFlowLogger, logger_params: Dict[str, Any] = {}, \n\u001b[0m\u001b[0;32m      3\u001b[0m                  callbacks: List[Callback] = [], callbacks_params: List[Dict[str, Any]] = [{}]):\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MLFlowLogger' is not defined"
     ]
    }
   ],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, obj_models: List[pl.LightningModule], version: str, folder_path: str = './', trainer_params: Dict[str, Any] = {}, logger: LightningLoggerBase = MLFlowLogger, logger_params: Dict[str, Any] = {}, \n",
    "                 callbacks: List[Callback] = [], callbacks_params: List[Dict[str, Any]] = [{}], datamodule_params: Dict[str, Any] = {}):\n",
    "        \"\"\"\n",
    "        Model saving path will be constructed via scheme:\n",
    "        path_to_folder+model_name/version+model.name\n",
    "        \"\"\"\n",
    "        self.models = obj_models\n",
    "        self.classifiers = [Classifier(model) for model in self.models]\n",
    "        self.version = version\n",
    "        split_names = []\n",
    "        for model in self.models:\n",
    "            model_name, _ = model.name.split('.')\n",
    "            split_names.append(model_name)\n",
    "        self.paths = [folder_path + model_name + '/' + version + model.name for model, name in zip(self.models, split_names)]\n",
    "        self.loggers = [logger(**logger_params) for _ in self.models] # Each model must get it's own logger for lightning trainer\n",
    "        self.trainer_params = trainer_params\n",
    "        self.callbacks = []\n",
    "        for _ in self.models: # Each model must get it's own set of callbacks, because these objects don't work well when reused\n",
    "            self.callbacks.append([callback(**params) for callback, params in zip(callbacks, callbacks_params)])\n",
    "        self.datamodules = [datamodule(**datamodule_params) for _ in self.models]\n",
    "        # Each model must have it's own datamodule that will be used for both fit AND test methods in lightning trainer\n",
    "    \n",
    "    def train(self, min_lr: float, datamodule: pl.LightningDataModule, transforms: List[Callable] = [], find_lr: bool = True, verbose: bool = False, no_batch: bool = False):\n",
    "        for path, classifier, logger, callbacks, datamodule in zip(self.paths, self.classifiers, self.loggers, self.callbacks, self.datamodules):\n",
    "            self.trainer = pl.Trainer(logger=logger, callbacks=callbacks, **self.trainer_params)\n",
    "            if find_lr:\n",
    "                lr_finder = self.trainer.tuner.lr_find(classifier, min_lr=min_lr, datamodule=datamodule(no_batch=no_batch, transforms=transforms), early_stop_threshold=None)\n",
    "                classifier.lr = lr_finder.suggestion()\n",
    "            else:\n",
    "                classifier.lr = min_lr\n",
    "\n",
    "            if verbose and find_lr:\n",
    "                print(f'Best lr: {classifier.lr}')\n",
    "\n",
    "            self.trainer.fit(classifier, datamodule=datamodule)\n",
    "            self.trainer.save_checkpoint(path)\n",
    "    \n",
    "    def test(self, datamodule: pl.LightningDataModule, transforms: List[Callable]):\n",
    "        for path, model, logger, datamodule in zip(self.paths, self.models, self.loggers, self.datamodules):\n",
    "            self.trainer = pl.Trainer(logger=logger, **self.trainer_params)\n",
    "            classifier = Classifier.load_from_checkpoint(path, model=model)\n",
    "            self.trainer.test(classifier, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d74ac-0ea4-4769-b454-c74505c93ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSearch:\n",
    "    def __init__(self, linear_params, linear_params_count):\n",
    "        self.linear_params = linear_params\n",
    "        self.linear_params_count = linear_params_count\n",
    "    \n",
    "    def search(self, models: List[pl.LightningModule], in_size: int, out_size: int, datamodule: pl.LightningDataModule, versions: List[str], min_lr: float = 1e-03, transforms: List[Callable] = [], \n",
    "               find_lr=False, verbose=False, callbacks: List[Callback] = [], callbacks_params: List[Dict[str, Any]] = [{}]):\n",
    "        for i in range(self.linear_params_count):\n",
    "            obj_models = [model(in_size, out_size) for model in models]\n",
    "            params = {}\n",
    "            for key in self.linear_params.keys():\n",
    "                params[key] = self.linear_params[key][i]\n",
    "            if verbose:\n",
    "                print(params)\n",
    "            evaluator = Evaluator(obj_models, versions[i], trainer_params=params, callbacks=callbacks, callbacks_params=callbacks_params)\n",
    "            evaluator.train(min_lr, datamodule, transforms, find_lr=find_lr, verbose=verbose)\n",
    "            evaluator.test(datamodule, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58a95b-d725-4fd1-ab2b-c3f397f8a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [] # Pass via class name\n",
    "min_lr = 1e-03\n",
    "in_size = 0\n",
    "out_size = 0\n",
    "datamodule = DataModule\n",
    "transforms = [normalize]\n",
    "callbacks = [] # List of callback class names\n",
    "callbacks_params = [{}] # List of corresponding parameters\n",
    "max_epochs = [20, 30, 40, 50, 75, 100]\n",
    "linear_params_count = len(max_epochs)\n",
    "linear_params = {\n",
    "    'gpus': generate(1, linear_params_count, 0),\n",
    "    'max_epochs': max_epochs,\n",
    "    'gradient_clip_val': generate(0.5, linear_params_count, 0.),\n",
    "    'stochastic_weight_avg': generate(True, linear_params_count, False),\n",
    "    'amp_level': generate('O3', linear_params_count, ''),\n",
    "    'precision': generate(16, linear_params_count, 0)\n",
    "}\n",
    "versions = [str(max_epoch) + 'epoch_' for max_epoch in params['max_epochs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243800c-d684-4fa8-a005-3833fff55255",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_search = LinearSearch(linear_params, linear_params_count)\n",
    "linear_search.search(models, in_size, out_size, datamodule, versions, min_lr, transforms, callbacks=callbacks, callbacks_params=callback_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec82901a-983c-434f-b042-3e79fdee71ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-3-48a8c77671c6>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-48a8c77671c6>\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    evaluator = Evaluator(obj_models=obj_models, version: str, folder_path: str = './', trainer_params: Dict[str, Any] = {},\u001b[0m\n\u001b[1;37m                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "class CrossValidation:\n",
    "    def __init__(self, n_splits, X, y, models, in_size, out_size, datamodule, trainer_params, logger, logger_params, callbacks):\n",
    "        self.skf = StratifiedKFold(n_split=n_splits)\n",
    "        self.models = models\n",
    "        self.obj_models = []\n",
    "        self.models_in_size = in_size\n",
    "        self.models_out_size = out_size\n",
    "        self.trainer_params = trainer_params\n",
    "        self.logger = logger\n",
    "        self.logger_params = logger_params\n",
    "        self.datamodule = datamodule\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        self.acc_scores = []\n",
    "        self.f1_scores = []\n",
    "    \n",
    "    def _prepare_data(self, train_idx, test_idx):\n",
    "        X_train, X_test = self.data[train_idx], self.data[test_idx]\n",
    "        y_train, y_test = self.target[train_idx], self.target[test_idx]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def _prepare_objects(self):\n",
    "        for model in slef.models:\n",
    "            self.obj_models.append(model(self.models_in_size, self.models_out_size))\n",
    "    \n",
    "    def call(self, version: str, folder_path: str = './'):\n",
    "        \"\"\"\n",
    "        Performs cross validation over given dataset. Uses LinearSearch for validating multiple models. \n",
    "        Params_count stands for length of list of one trainer parameter.\n",
    "        Saves models after each k-th fold via scheme:\n",
    "        k+version+\n",
    "        \"\"\"\n",
    "        for i, idx in enumerate(self.skf.split(X, y)):\n",
    "            train_idx, test_idx = idx\n",
    "            X_train, X_test, y_train, y_test = self.prepare_data(train_idx, test_idx)\n",
    "            obj_models = [model(self.models_in_size, self.models_out_size) for model in self.models]\n",
    "            datamodule = self.datamodule\n",
    "            loop_version = str(i) + '_' + version\n",
    "            evaluator_params = {\n",
    "                'obj_models': obj_models,\n",
    "                'version': loop_version,\n",
    "                'folder_path': folder_path,\n",
    "                'trainer_params': trainer_params,\n",
    "                'logger': logger,\n",
    "                'logger_params': logger_params,\n",
    "                'callbacks': = \n",
    "            }\n",
    "            evaluator = Evaluator(obj_models=obj_models, version: str, folder_path: str = './', trainer_params: Dict[str, Any] = {}, \n",
    "                                  logger: LightningLoggerBase = MLFlowLogger, logger_params: Dict[str, Any] = {}, \n",
    "                 callbacks: List[Callback] = [], callbacks_params: List[Dict[str, Any]] = [{}], datamodule_params: Dict[str, Any] = {})\n",
    "            for j, metrics in enumerate(zip(linear_search.acc_scores, linear_search.f1_scores)):\n",
    "                acc_scores[j].append(metrics[0]) # 9 list po 10 słowników po 2 modele\n",
    "                f1_scores[j].append(metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2ef7d-25b1-4a5f-a173-6132cdf9cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "datamodule = PimaIndiansDataModule(no_batch=True, transforms=[normalize])\n",
    "datamodule.prepare_data()\n",
    "X = datamodule.transformed_data\n",
    "y = datamodule.full_data.values[:, -1]\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "for _ in range(params_count):\n",
    "    acc_scores.append(list())\n",
    "    f1_scores.append(list())\n",
    "    \n",
    "for i, idx in enumerate(skf.split(X, y)):\n",
    "    train_idx, test_idx = idx\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    train_dataset = pd.DataFrame(X_train)\n",
    "    train_dataset['Output'] = y_train\n",
    "    test_dataset = pd.DataFrame(X_test)\n",
    "    test_dataset['Output'] = y_test\n",
    "    datamodule = DataModule\n",
    "    linear_search = LinearSearch(params, params_count)\n",
    "    loop_versions = [str(i) + '_' + version for version in versions]\n",
    "    linear_search.search(models, in_size, out_size, datamodule, loop_versions, min_lr, transforms, False, False, callbacks, early_stopping_params, train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "    for j, metrics in enumerate(zip(linear_search.acc_scores, linear_search.f1_scores)):\n",
    "        acc_scores[j].append(metrics[0]) # 9 list po 10 słowników po 2 modele\n",
    "        f1_scores[j].append(metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999a5d1-c8ff-43b8-ae01-e7674fdf8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = acc_scores[0][0].keys()\n",
    "final_models_acc_scores = []\n",
    "final_models_f1_scores = []\n",
    "for acc, f1 in zip(acc_scores, f1_scores):\n",
    "    for model in models:\n",
    "        model_acc_scores = [models_dict[model] for models_dict in acc]\n",
    "        model_f1_scores = [models_dict[model] for models_dict in f1]\n",
    "        final_models_acc_scores.append(np.mean(model_acc_scores))\n",
    "        final_models_f1_scores.append(np.mean(model_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d788588-bd65-4ee4-96c7-b9045d056e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = './model/'\n",
    "datamodule = PimaIndiansDataModule\n",
    "networks = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "params = {\n",
    "    'gpus': 1,\n",
    "    'amp_level': 'O3',\n",
    "    'precision': 16\n",
    "}\n",
    "for network in networks:\n",
    "    try:\n",
    "        file_name = network\n",
    "        network = mypath + network\n",
    "        if 'double' in network:\n",
    "            print('Double Big: ' + file_name)\n",
    "            model = Classifier.load_from_checkpoint(network, model=SpreadNNNoPoolDoubleBig(in_size, out_size))\n",
    "            trainer = pl.Trainer(logger=MLFlowLogger(), **params)\n",
    "            trainer.test(model, datamodule=datamodule(no_batch=True, transforms=transforms))\n",
    "        else:\n",
    "            print('Big: ' + file_name)\n",
    "            model = Classifier.load_from_checkpoint(network, model=SpreadNNNoPoolBig(in_size, out_size))\n",
    "            trainer = pl.Trainer(logger=MLFlowLogger(), **params)\n",
    "            trainer.test(model, datamodule=datamodule(no_batch=True, transforms=transforms))\n",
    "    except:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
