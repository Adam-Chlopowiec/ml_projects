{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68664161-6341-410b-9533-421707d06d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sklearn\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "from pytorch_lightning.loggers.base import LightningLoggerBase\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional, List, Dict, Any, Callable\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pl.utilities.seed.seed_everything(42)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece627e-266b-407b-85c8-caead7e3e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def standarize(data):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240d5d9-b20d-4dac-98f0-47b06b0032d7",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897f617-83d6-443e-98b9-ddd62bedc897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PimaIndiansDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"data/diabetes.csv\", train_batch_size: int = 64, val_batch_size: int = 64, transforms: List[Callable] = [], no_batch: bool = False):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.no_batch = no_batch\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.full_data = pd.read_csv('data/diabetes.csv')\n",
    "        #self.full_data = self.full_data.drop(['Age'], axis=1)\n",
    "        X = self.full_data.values[:, :-1]\n",
    "        y = self.full_data.values[:, -1]\n",
    "        for transform in self.transforms:\n",
    "            X = transform(X)\n",
    "        self.transformed_data = X\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "        X_train = torch.tensor(X_train).float()\n",
    "        X_test = torch.tensor(X_test).float()\n",
    "        y_train = torch.tensor(y_train).long()\n",
    "        y_test = torch.tensor(y_test).long()\n",
    "        \n",
    "        self.train_data = (X_train, y_train)\n",
    "        self.test_data = (X_test, y_test)\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            X_train, y_train = self.train_data\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)\n",
    "            self.train_data = []\n",
    "            self.val_data = []\n",
    "            for x, y in zip(X_train, y_train):\n",
    "                self.train_data.append((x, y))\n",
    "                \n",
    "            for x, y in zip(X_val, y_val):\n",
    "                self.val_data.append((x, y))\n",
    "            \n",
    "            if self.no_batch:\n",
    "                self.train_batch_size = len(self.train_data)\n",
    "                self.val_batch_size = len(self.val_data)\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            X_test, y_test = self.test_data\n",
    "            self.test_data = []\n",
    "            for x, y in zip(X_test, y_test):\n",
    "                self.test_data.append((x, y))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, self.train_batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, self.val_batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, shuffle=False, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5501693-3006-47bb-b8b6-cf2c53cc742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = PimaIndiansDataModule(no_batch=True, transforms=[normalize])\n",
    "datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9bb6f-11be-4923-b813-2ba97b7a1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "values = pca.fit_transform(datamodule.transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5657fd-7022-47b8-9126-b1519b46a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = pca.explained_variance_ratio_\n",
    "gen = iter(range(0, 9, 1))\n",
    "labels = [next(gen) for _ in heights]\n",
    "plt.bar(labels, heights)\n",
    "len(heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d74ca-5dda-4149-b40c-a52015c67dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.full_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22daf67-711d-4138-a463-d216f4290fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63d6cb-b647-4330-941e-280972d5059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.full_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72cf05a-1361-43f0-a820-f524453026b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_class = sum(datamodule.full_data.values[:, -1])\n",
    "zero_class = datamodule.full_data['Outcome'].size - one_class\n",
    "print(f'Class 0: {zero_class}')\n",
    "print(f'Class 1: {one_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7253d-9f11-4feb-920f-89179e68ce98",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d519ce4-0be9-4874-8726-77a31c5673ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(data, x, y, figsize=(13, 13), bins=15):\n",
    "    fig, ax = plt.subplots(x, y, figsize=figsize)\n",
    "    for i in range(data.transformed_data.shape[1]):\n",
    "        ax[int(i / y), i % y].hist(data.transformed_data[:, i], bins=bins)\n",
    "        ax[int(i / y), i % y].set_title(data.full_data.columns[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bfd15-2b02-4c7f-b1d3-cb29c8fc9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(datamodule, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484dca68-0263-4106-8aeb-b1677a77d652",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeab466-3e37-4d0b-8680-38a2f12e1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.test_acc = 0\n",
    "        self.test_f1 = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "        \n",
    "    def shared_step(self, batch):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(pred, y)\n",
    "        return pred, loss\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        pred, loss = self.shared_step(train_batch)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        _, y = train_batch\n",
    "        return {'loss': loss, 'train_score': (predicted, y)}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = [output['loss'] for output in outputs]\n",
    "        loss_nd = np.double(loss[-1].detach().cpu().numpy())\n",
    "        \n",
    "        results = [x['train_score'] for x in outputs]\n",
    "        preds = []\n",
    "        y = []\n",
    "        for predicted, y_data in results:\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            y_data = y_data.detach().cpu().numpy()\n",
    "            preds.extend(predicted)\n",
    "            y.extend(y_data)\n",
    "        f1 = f1_score(y, preds)\n",
    "        self.log('loss', loss_nd, logger=True)\n",
    "        self.log('train_f1', f1, prog_bar=True, logger=True)\n",
    "    \n",
    "    def validation_step(self, val_batch, val_batch_idx):\n",
    "        pred, loss = self.shared_step(val_batch)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        _, y = val_batch\n",
    "        return {'val_loss': loss, 'val_score': (predicted, y)}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        results = [x['val_score'] for x in outputs]\n",
    "        preds = []\n",
    "        y = []\n",
    "        for predicted, y_data in results:\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            y_data = y_data.detach().cpu().numpy()\n",
    "            preds.extend(predicted)\n",
    "            y.extend(y_data)\n",
    "        f1 = f1_score(y, preds)\n",
    "        self.log('val_f1', f1, prog_bar=True, logger=True)\n",
    "        \n",
    "    def test_step(self, test_batch, test_batch_idx):\n",
    "        pred, loss = self.shared_step(test_batch)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        _, y = test_batch\n",
    "        return {'test_loss': loss, 'test_score': (predicted, y)}\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        results = [x['test_score'] for x in outputs]\n",
    "        preds = []\n",
    "        y = []\n",
    "        for predicted, y_data in results:\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            y_data = y_data.detach().cpu().numpy()\n",
    "            preds.extend(predicted)\n",
    "            y.extend(y_data)\n",
    "        acc = accuracy_score(y, preds)\n",
    "        f1 = f1_score(y, preds)\n",
    "        self.test_acc = acc\n",
    "        self.test_f1 = f1\n",
    "        self.log('accuracy', acc, prog_bar=True, logger=True)\n",
    "        self.log('f1', f1, prog_bar=True, logger=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6dc68-5871-4e16-86aa-016b69ac7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadNNNoPool(pl.LightningModule):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size * 2),\n",
    "            nn.BatchNorm1d(in_size * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size * 2, in_size * 4),\n",
    "            nn.BatchNorm1d(in_size * 4),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size * 4, in_size * 2),\n",
    "            nn.BatchNorm1d(in_size * 2),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size * 2, int(in_size / 2)),\n",
    "            nn.BatchNorm1d(int(in_size / 2)),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(int(in_size / 2), out_size)\n",
    "        )\n",
    "        \n",
    "        self.name = 'spreadnn_no_pool.ckpt'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ae821-46f2-4c33-ac7f-84b8108013a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadNNNoPoolBig(pl.LightningModule):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        in_size_2 = in_size * 2\n",
    "        in_size_4 = in_size * 4\n",
    "        in_size_8 = in_size * 8\n",
    "        in_size_16 = in_size * 16\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size_4),\n",
    "            nn.BatchNorm1d(in_size_4),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_4, in_size_16),\n",
    "            nn.BatchNorm1d(in_size_16),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_16, in_size_8),\n",
    "            nn.BatchNorm1d(in_size_8),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_8, in_size_4),\n",
    "            nn.BatchNorm1d(in_size_4),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_4, in_size),\n",
    "            nn.BatchNorm1d(in_size),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size, out_size)\n",
    "        )\n",
    "        \n",
    "        self.name = 'spreadnn_no_pool_big.ckpt'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c7fd9-ba58-4c3f-9225-19ecac9e65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadNNNoPoolGigaBig(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(8, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(32, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(32, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(8, 2)\n",
    "        )\n",
    "        \n",
    "        self.name = 'spreadnn_no_pool_giga_big.ckpt'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1202bb5-b810-46ba-b4a4-9af2ab0a0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadNNNoPoolDoubleBig(pl.LightningModule):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        in_size_2 = in_size * 2\n",
    "        in_size_4 = in_size * 4\n",
    "        in_size_8 = in_size * 8\n",
    "        in_size_16 = in_size * 16\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size_4),\n",
    "            nn.BatchNorm1d(in_size_4),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_4, in_size_16),\n",
    "            nn.BatchNorm1d(in_size_16),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_16, in_size_16),\n",
    "            nn.BatchNorm1d(in_size_16),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_16, in_size_8),\n",
    "            nn.BatchNorm1d(in_size_8),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_8, in_size_4),\n",
    "            nn.BatchNorm1d(in_size_4),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size_4, in_size),\n",
    "            nn.BatchNorm1d(in_size),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(in_size, out_size)\n",
    "        )\n",
    "        \n",
    "        self.name = 'spreadnn_no_pool_double_big.ckpt'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626cbd4a-51b0-47cb-910e-34e63619c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpreadNNSmall(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(8, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(16, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(8, 4),\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.LeakyReLU(),\n",
    "            \n",
    "            nn.Linear(4, 2)\n",
    "        )\n",
    "        \n",
    "        self.name = 'spreadnn_small.ckpt'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._nn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4174bb-4ab0-4337-a884-70318c6dfe4a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dbfe4-2e48-4e58-8dda-6aa57746d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(elem, count, step):\n",
    "    current = elem\n",
    "    output = []\n",
    "    for _ in range(count):\n",
    "        output.append(current)\n",
    "        current += step\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850dcb23-5192-4ff9-80dc-b1ae430c0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, models: List[pl.LightningModule], version: str, trainer_params: Dict[str, Any] = {}, logger: LightningLoggerBase = MLFlowLogger, logger_params: Dict[str, Any] = {},\n",
    "                 callbacks=[], callback_params={}, train_dataset = None, test_dataset = None):\n",
    "        self.models = models\n",
    "        self.classifiers = [Classifier(model) for model in models]\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.version = version\n",
    "        self.paths = ['model/k_fold/' + version + model.name for model in models]\n",
    "        self.loggers = [logger(**logger_params) for _ in models]\n",
    "        self.callbacks = []\n",
    "        for _ in models:\n",
    "            self.callbacks.append([callback(**callback_params) for callback in callbacks])\n",
    "        self.trainer_params = trainer_params\n",
    "        self.acc_scores = {}\n",
    "        self.f1_scores = {}\n",
    "        for model in models:\n",
    "            self.acc_scores[model.name] = []\n",
    "            self.f1_scores[model.name] = []\n",
    "    \n",
    "    def train(self, min_lr: float, datamodule: pl.LightningDataModule, transforms: List[Callable], find_lr: bool = True, verbose: bool = False):\n",
    "        for path, classifier, logger, callback in zip(self.paths, self.classifiers, self.loggers, self.callbacks):\n",
    "            self.trainer = pl.Trainer(logger=logger, callbacks=callback, **self.trainer_params)\n",
    "            if find_lr:\n",
    "                lr_finder = self.trainer.tuner.lr_find(classifier, min_lr=min_lr, datamodule=datamodule(train_dataset, test_dataset), early_stop_threshold=None)\n",
    "                classifier.lr = lr_finder.suggestion()\n",
    "            else:\n",
    "                classifier.lr = min_lr\n",
    "\n",
    "            if verbose and find_lr:\n",
    "                print(f'Best lr: {classifier.lr}')\n",
    "\n",
    "            self.trainer.fit(classifier, datamodule(train_dataset, test_dataset))\n",
    "            self.trainer.save_checkpoint(path)\n",
    "    \n",
    "    def test(self, datamodule: pl.LightningDataModule, transforms: List[Callable]):\n",
    "        for path, model, logger in zip(self.paths, self.models, self.loggers):\n",
    "            self.trainer = pl.Trainer(logger=logger, **self.trainer_params)\n",
    "            classifier = Classifier.load_from_checkpoint(path, model=model)\n",
    "            self.trainer.test(classifier, datamodule=datamodule(train_dataset, test_dataset))\n",
    "            self.acc_scores[classifier.model.name].append(classifier.test_acc)\n",
    "            self.f1_scores[classifier.model.name].append(classifier.test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d788588-bd65-4ee4-96c7-b9045d056e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSearch:\n",
    "    def __init__(self, grid_params, combinations):\n",
    "        self.grid_params = grid_params\n",
    "        self.combinations = combinations\n",
    "        self.acc_scores = []\n",
    "        self.f1_scores = []\n",
    "    \n",
    "    def search(self, models, in_size, out_size, datamodule, versions, min_lr, transforms, find_lr, verbose, callbacks=[], callback_params={}, train_dataset = None, test_dataset = None):\n",
    "        for i in range(self.combinations):\n",
    "            obj_models = [model(in_size, out_size) for model in models]\n",
    "            params = {}\n",
    "            for key in self.grid_params.keys():\n",
    "                params[key] = self.grid_params[key][i]\n",
    "            print(params)\n",
    "            evaluator = Evaluator(obj_models, versions[i], params, callbacks=callbacks, callback_params=callback_params, train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "            evaluator.train(min_lr, datamodule, transforms, find_lr=find_lr, verbose=verbose)\n",
    "            evaluator.test(datamodule, transforms)\n",
    "            self.acc_scores.append(evaluator.acc_scores)\n",
    "            self.f1_scores.append(evaluator.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ef745-9c94-4401-b917-f4f3d2ac1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [SpreadNNNoPoolBig, SpreadNNNoPoolDoubleBig]\n",
    "min_lr = 1e-03\n",
    "in_size = 8\n",
    "out_size = 2\n",
    "datamodule = PimaIndiansDataModule\n",
    "transforms = [normalize]\n",
    "early_stopping_params = {'monitor': 'loss', 'min_delta': 0.001, 'patience': 6}\n",
    "callbacks = [EarlyStopping]\n",
    "params_count = 3\n",
    "params = {\n",
    "    'gpus': generate(1, params_count, 0),\n",
    "    'max_epochs': [30, 75, 100],\n",
    "    'gradient_clip_val': generate(0.5, params_count, 0.),\n",
    "    'stochastic_weight_avg': generate(True, params_count, False),\n",
    "    'amp_level': generate('O3', params_count, ''),\n",
    "    'precision': generate(16, params_count, 0)\n",
    "}\n",
    "versions = [str(max_epoch) + 'epoch_' for max_epoch in params['max_epochs']]\n",
    "print(versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bc6a2-9f91-4ec1-b10d-648733e7e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    linear_search = LinearSearch(params, params_count)\n",
    "    loop_versions = [str(i) + '_' + version for version in versions]\n",
    "    linear_search.search(models, in_size, out_size, datamodule, loop_versions, min_lr, transforms, False, False, callbacks, early_stopping_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c5c2e-532d-433a-955b-a0a1c2fe22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, test_dataset):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        train = []\n",
    "        X_train = torch.tensor(self.train_dataset.values[:, :-1]).float()\n",
    "        y_train = torch.tensor(self.train_dataset.values[:, -1]).long()\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            train.append((x, y))\n",
    "        self.train_dataset = train\n",
    "        \n",
    "        test = []\n",
    "        X_test = torch.tensor(self.test_dataset.values[:, :-1]).float()\n",
    "        y_test = torch.tensor(self.test_dataset.values[:, -1]).long()\n",
    "        for x, y in zip(X_test, y_test):\n",
    "            test.append((x, y))\n",
    "        self.test_dataset = test\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, len(self.train_dataset), shuffle=True, num_workers=1, pin_memory=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, len(self.test_dataset), shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3fef8-8b24-479f-b2e8-8d5ddbc0db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "datamodule = PimaIndiansDataModule(no_batch=True, transforms=[normalize])\n",
    "datamodule.prepare_data()\n",
    "X = datamodule.transformed_data\n",
    "y = datamodule.full_data.values[:, -1]\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "for _ in range(params_count):\n",
    "    acc_scores.append(list())\n",
    "    f1_scores.append(list())\n",
    "    \n",
    "for i, idx in enumerate(skf.split(X, y)):\n",
    "    train_idx, test_idx = idx\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    train_dataset = pd.DataFrame(X_train)\n",
    "    train_dataset['Output'] = y_train\n",
    "    test_dataset = pd.DataFrame(X_test)\n",
    "    test_dataset['Output'] = y_test\n",
    "    datamodule = DataModule\n",
    "    linear_search = LinearSearch(params, params_count)\n",
    "    loop_versions = [str(i) + '_' + version for version in versions]\n",
    "    linear_search.search(models, in_size, out_size, datamodule, loop_versions, min_lr, transforms, False, False, callbacks, early_stopping_params, train_dataset=train_dataset, test_dataset=test_dataset)\n",
    "    for j, metrics in enumerate(zip(linear_search.acc_scores, linear_search.f1_scores)):\n",
    "        acc_scores[j].append(metrics[0]) # 9 list po 10 słowników po 2 modele\n",
    "        f1_scores[j].append(metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db898f7-57c5-4633-8b50-6ff3030a67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = acc_scores[0][0].keys()\n",
    "final_models_acc_scores = []\n",
    "final_models_f1_scores = []\n",
    "for acc, f1 in zip(acc_scores, f1_scores):\n",
    "    for model in models:\n",
    "        model_acc_scores = [models_dict[model] for models_dict in acc]\n",
    "        model_f1_scores = [models_dict[model] for models_dict in f1]\n",
    "        final_models_acc_scores.append(np.mean(model_acc_scores))\n",
    "        final_models_f1_scores.append(np.mean(model_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e938b-aeb0-4298-9c57-3983003c6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acc, f1 in zip(final_models_acc_scores, final_models_f1_scores):\n",
    "    result = {'accuracy': acc, 'f1': f1}\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
